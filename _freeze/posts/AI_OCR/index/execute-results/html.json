{
  "hash": "fefa4dd1a4f779e86c3f31f9194e8cf8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AI图片识别文字\"\nsubtitle: \"AI Optical character recognition\"\nauthor: \"Tony D\"\ndate: \"2025-04-21\"\n\ncategories: \n  - AI\n  - R\n  - Python\n\n  \nexecute:\n  warning: false\n  error: false\n  \n  \nimage: 'images/001.png'\n---\n\nwith Gemini 2.5 online/InternVL3 offline\n\n\n\n# Using Gemini 2.5 online\n\n::: {#4dfe9575 .cell execution_count=1}\n``` {.python .cell-code}\nfrom google import genai\nimport keyring\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\nimport sys\nimport pickle\nimport math\nimport numpy as np\nimport torch\n```\n:::\n\n\n\n\n::: {#7ac497d1 .cell execution_count=3}\n``` {.python .cell-code}\nclient = genai.Client(api_key=keyring.get_password(\"system\", \"google_ai_api_key\"))\n```\n:::\n\n\nlist model\n\n::: {#943a0cba .cell execution_count=4}\n``` {.python .cell-code}\nprint(\"List of models:\\n\")\nfor m in client.models.list():\n    for action in m.supported_actions:\n       # if action == \"generateContent\":\n            print(m.name+\" \"+ action)\n```\n:::\n\n\n## English Extract\n\n\n\n![](images/english.jpg)\n\n::: {#eee3db2f .cell execution_count=5}\n``` {.python .cell-code}\nimage = Image.open(\"images/english.jpg\")\n\nresponse_gemini_en = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=[image, \"Extract text from image\"])\n```\n:::\n\n\n::: {#0b634cd9 .cell execution_count=6}\n``` {.python .cell-code}\nprint(response_gemini_en.text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWrite slowly and take the time\nto make sure each letter\nis the perfect shape\n```\n:::\n:::\n\n\n## chinese Extract\n\n![](images/chinese.png)\n\n::: {#4db96fa3 .cell execution_count=7}\n``` {.python .cell-code}\nimage = Image.open(\"images/chinese.png\")\n\nresponse_gemini = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=[image, \"提取图上的文字\"])\n```\n:::\n\n\n::: {#1079c1ae .cell execution_count=8}\n``` {.python .cell-code}\nprint(response_gemini.text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n放养\n\n把我不羁的灵魂\n放养在可可西里的草原上，\n藏雪狐活泼没人爱人\n我与它捉迷藏\n\n把我不羁的灵魂，\n放养在撒哈拉沙漠上，\n看生命在贫瘠的土地上，\n依然欣欣向荣地生长\n\n把我不羁的灵魂\n放养在昏黄遗迹的小岛\n坐上星期五的木筏\n勇敢地乘风破浪\n\n把我不羁的灵魂\n放养在天涯海角\n就让我自由地去流浪。\n```\n:::\n:::\n\n\n# Using InternVL3 1B model offline\n\nhttps://huggingface.co/OpenGVLab/InternVL3-8B-hf\n\nIf using better model will increase accuracy\n\n::: {#780938cd .cell execution_count=9}\n``` {.python .cell-code}\nprint(sys.version)\n```\n:::\n\n\n::: {#36c3ef9f .cell execution_count=10}\n``` {.python .cell-code}\npip install --upgrade transformers\npip install einops timm\npip install -U bitsandbytes\n```\n:::\n\n\n::: {#b16dbacd .cell execution_count=11}\n``` {.python .cell-code}\nimport os\nos.system('pip show transformers')\n```\n:::\n\n\n::: {#ef91e1c3 .cell execution_count=12}\n``` {.python .cell-code}\nimport torch\nfrom transformers import AutoTokenizer, AutoModel,pipeline\npath = \"OpenGVLab/InternVL3-1B\"\n\nmodel = AutoModel.from_pretrained(path,torch_dtype=torch.bfloat16,trust_remote_code=True).eval()\n\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n```\n:::\n\n\n::: {#a274fd09 .cell execution_count=13}\n``` {.python .cell-code}\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n```\n:::\n\n\ntesting\n\n::: {#293953a7 .cell execution_count=14}\n``` {.python .cell-code}\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n```\n:::\n\n\n## single-image single-round conversation (单图单轮对话)\n\n::: {#07a94af5 .cell execution_count=15}\n``` {.python .cell-code}\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\ndef split_model(model_name):\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n    num_layers = config.llm_config.num_hidden_layers\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n            layer_cnt += 1\n    device_map['vision_model'] = 0\n    device_map['mlp1'] = 0\n    device_map['language_model.model.tok_embeddings'] = 0\n    device_map['language_model.model.embed_tokens'] = 0\n    device_map['language_model.output'] = 0\n    device_map['language_model.model.norm'] = 0\n    device_map['language_model.model.rotary_emb'] = 0\n    device_map['language_model.lm_head'] = 0\n    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n\n    return device_map\n\n```\n:::\n\n\n### English Extract\n\n![](images/english.jpg)\n\n::: {#bae56d88 .cell execution_count=16}\n``` {.python .cell-code}\npixel_values = load_image('images/english.jpg').to(torch.bfloat16)\nquestion = '<image>\\nPlease Extract text from image'\nresponse_en = model.chat(tokenizer, pixel_values, question, generation_config)\n```\n:::\n\n\n::: {#1ad024eb .cell execution_count=17}\n``` {.python .cell-code}\nprint(response_en)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWrite slowly and take the time to make sure each letter is the perfect shape\n```\n:::\n:::\n\n\n### Chinese Extract\n\n![](images/chinese.png)\n\n::: {#89c3181f .cell execution_count=18}\n``` {.python .cell-code}\npixel_values = load_image('images/chinese.png').to(torch.bfloat16)\n\nquestion = '<image>\\n提取图上的文字'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\n```\n:::\n\n\n::: {#760ae8a2 .cell execution_count=19}\n``` {.python .cell-code}\nprint(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n放养  \n把我不羁的灵魂  \n放在了鄂西西里的草原上，  \n藏雪狐活发爱人  \n我与它捉迷藏  \n把我不羁的灵魂  \n放在撒哈拉沙漠上，  \n看生命在贫瘠的土地上，  \n依然欣欣欣荣地生长  \n把我不羁的灵魂  \n放在鲁滨逊的小岛上  \n坐上星期五的木筏  \n勇敢地乘风破浪  \n\n把我不羁的灵魂  \n放养在天涯海角  \n让我自由地去流浪。\n```\n:::\n:::\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}