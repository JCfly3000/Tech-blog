{
  "hash": "ced51b70183226c2cdcb6f6e7ae4aeed",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"（LLM）大语言模型\"\nsubtitle: \"(LLM)Large language model\"\nauthor: \"Tony D\"\n\ndate: \"2025-03-18\"\n\ncategories: \n  - Tool\n  - R\n  - Python \n\n  \nexecute:\n  warning: false\n  error: false\n  eval: false\n\nimage: '1734616066-llm-security.webp'\n---\n\nAn overview of Large Language Models (LLMs) and their performance across various benchmarks, including math, code, English, and science.\n\nThis document provides a comprehensive overview of Large Language Models (LLMs) and their performance across a variety of benchmarks. It categorizes the performance of different models by subject area, including mathematics, coding, English language understanding, and science. The document also provides links to resources where you can compare the performance of different LLM models online.\n\n(LLM)Large language model\n\n![](images/my%20screenshots.png){width=\"800\"} \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(openxlsx)\nlibrary(readxl)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata001=read_excel('AI model.xlsx')\nhead(data001)\n```\n:::\n\n# LLM model performance\n\n## math\n\n### AIME\n\nhttps://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination\n\n### MATH-500\n\n## Code\n\n## Codeforces\n\n## LiveCodeBench\n\n## English\n\n### MMLU\n\nMeasuring Massive Multitask Language Understanding (MMLU)\n\nhttps://en.wikipedia.org/wiki/MMLU\n\n## Science\n\n### GPQA-Diamond\n\nGraduate-Level Google-Proof Q&A\n\nDescription: GPQA consists of 448 multiple-choice questions meticulously crafted by domain experts in biology, physics, and chemistry. These questions are intentionally designed to be high-quality and extremely difficult.\n\nExpert Accuracy: Even experts who hold or are pursuing PhDs in the corresponding domains achieve only 65% accuracy on these questions (or 74% when excluding clear mistakes identified in retrospect).\n\nGoogle-Proof: The questions are \"Google-proof,\" meaning that even with unrestricted access to the web, highly skilled non-expert validators only reach an accuracy of 34% despite spending over 30 minutes searching for answers.\n\nAI Systems Difficulty: State-of-the-art AI systems, including our strongest GPT-4 based baseline, achieve only 39% accuracy on this challenging dataset.\n\n# SimpleBench\n\nhttps://simple-bench.com/\n\n![](images/clipboard-1311795434.png)\n# Compare online\n\n\nhttps://lmarena.ai/",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}