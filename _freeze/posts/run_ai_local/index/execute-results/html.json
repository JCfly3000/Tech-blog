{
  "hash": "1929cd9194bada5bd053fdbe054cd4fb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"本地运行AI模型\"\nsubtitle: \"Run AI model on local machine\"\nauthor: \"Tony D\"\ndate: \"2025-03-18\"\n\ncategories: \n  - AI\n  - R\n  - Python\n  \nimage: \"images/1719563355.png\"\n\nexecute:\n  warning: false\n  error: false\n  eval: false\n---\n\nRunning LLM model on local machine with Ollama,huggingface and more\n\n\n# Ollama\n\n\n![](images/clipboard-1677176147.png)\n\n## Download and install the Ollama app\n\nhttps://ollama.com/download\n\nand open the app on computer\n\n## Run LLM model on Ollama\n\n::: panel-tabset\n\n### Run in R with ollamar pacakge\n\n#### download pacakge check connection\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ollamar\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ollamar)\ntest_connection() \n```\n:::\n\n#### download model\n\n::: {.cell}\n\n```{.r .cell-code}\nollamar::pull(\"llama3.1\")\n```\n:::\n\n#### list downloaded model\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_models()\n```\n:::\n\n#### show model detail\n\n::: {.cell}\n\n```{.r .cell-code}\n#ollamar::show(\"llama3.1\")\n```\n:::\n\n#### run model\n\n::: {.cell}\n\n```{.r .cell-code}\nresp <- generate(\"llama3.1\", \"tell me a 5-word story\")\nresp\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get just the text from the response object\nresp_process(resp, \"text\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the text as a tibble dataframe\nresp_process(resp, \"df\")\n```\n:::\n\n\n### Run in R with ellmer package\n\n\n\n### Run in terminal\n\n\n#### download model\n\n::: {.cell}\n\n```{.python .cell-code}\n!ollama pull llama3.1\n```\n:::\n\n\n#### run model\n\n::: {.cell}\n\n```{.python .cell-code}\n!ollama run llama3.1 \"tell me a 5-word story\"\n```\n:::\n\n\n\n#### Run in Python\n\n::: {.cell}\n\n```{.python .cell-code}\n!pip install ollama\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom ollama import chat\nfrom ollama import ChatResponse\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport ollama\n```\n:::\n\n\n#### download model\n\n::: {.cell}\n\n```{.r .cell-code}\nollama.pull(\"llama3.1\")\n```\n:::\n\n\n#### show downloaded model\n::: {.cell}\n\n```{.python .cell-code}\nollama.list()\n```\n:::\n\n#### Run model\n::: {.cell}\n\n```{.python .cell-code}\nollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': 'who are you?'}])\n```\n:::\n#### create a model with prompt\n\n::: {.cell}\n\n```{.python .cell-code}\nollama.create(model='Mario', from_='llama3.1', system=\"You are Mario from Super Mario Bros.\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nollama.chat(model='Mario', messages=[{'role': 'user', 'content': 'who are you?'}])\n```\n:::\n#### delete model\n\n::: {.cell}\n\n```{.python .cell-code}\nstatus = ollama.delete('example')\nstatus\n```\n:::\n\n\n:::\n\n\n# hugging face\n\n::: panel-tabset\n\n## Run in Python with hugging face\n\nUsing mlx DeepSeek-R1-4bit as example :https://huggingface.co/mlx-community/DeepSeek-R1-4bit\n\nUsing python 3.11 \n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(RETICULATE_PYTHON = \"/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11\")\nlibrary(reticulate)\nuse_python(\"/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom platform import python_version\nprint(python_version())\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\n!pip3.11 install mlx-lm\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"mlx-community/DeepSeek-R1-4bit\")\n\nprompt = \"hello\"\n\nif tokenizer.chat_template is not None:\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    prompt = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True\n    )\n\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)\nresponse\n```\n:::\n\n:::\n\n\n# Terminal \n\n::: panel-tabset\n\n\n\n## run in Terminal\n\nruning mlx_whisper as example\n\nDownload model\n\n::: {.cell}\n\n```{.python .cell-code}\n!brew install ffmpeg\n!pip install mlx-whisper\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\n!mlx_whisper xxxx.mp3 --model mlx-community/whisper-turbo --language 'Chinese' --initial-prompt '以下是普通話的句子,請以繁體輸出'\n```\n:::\n\nmlx_whisper xxxx.mp3 --model mlx-community/whisper-turbo --language 'Chinese' --initial-prompt '以下是普通話的句子,請以繁體輸出'\n\n## run in R code\n\n::: {.cell}\n\n```{.r .cell-code}\ncommand=paste0(\"mlx_whisper '\",file_name,\"' --model mlx-community/whisper-turbo --language 'Chinese' --initial-prompt '以下是普通話的句子,請以繁體輸出'\")\n\ncommand\n```\n:::\n\n## run in Python code \n\n::: {.cell}\n\n```{.r .cell-code}\nimport os\n\ncommand\nos.system(command)\n```\n:::\n\n\n\n:::\n\n\n\n\n# mall pacakge \n\n\nhttps://mlverse.github.io/mall/\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}