{
  "hash": "c8994e1fbaa82c2b66caa992672dc283",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"本地运行AI模型\"\nsubtitle: \"Run AI model on local machine\"\nauthor: \"Tony D\"\ndate: \"2025-03-18\"\n\ncategories: \n  - AI\n  - R\n  - Python\n  \nimage: \"images/1719563355.png\"\n\nexecute:\n  warning: false\n  error: false\n  eval: false\n---\n\nRunning AI model on local machine with Ollama, huggingface and more\n\n# 1.Ollama\n\n![](images/clipboard-3590619259.png)\n\n## Download and install the Ollama app\n\nhttps://ollama.com/download\n\nand open the app on computer\n\n## Run LLM model on Ollama\n\n::: panel-tabset\n\n### Run in R with ollamar pacakge\n\n#### download pacakge check connection\n\n::: {.cell}\n\n```{.r .cell-code}\npak::pak(\"ollamar\")\npak::pkg_deps_tree(\"ollamar\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ollamar)\ntest_connection() \n```\n:::\n\ndownload model\n\n::: {.cell}\n\n```{.r .cell-code}\nollamar::pull(\"llama3.1\")\n```\n:::\n\nlist downloaded model\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_models()\n```\n:::\n\nshow model detail\n\n::: {.cell}\n\n```{.r .cell-code}\nollamar::show(\"gemma3\")\n```\n:::\n\nrun model\n\n::: {.cell}\n\n```{.r .cell-code}\nresp <- generate(\"gemma3\", \"tell me a 5-word story\")\nresp\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get just the text from the response object\nresp_process(resp, \"text\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the text as a tibble dataframe\nresp_process(resp, \"df\")\n```\n:::\n\nusing multiple models\n\n::: {.cell}\n\n```{.r .cell-code}\n(list_models())$name\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels_name=(list_models())$name[-1]\nmodels_name\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_prompt=\"tell me a 5-word story\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_model=c()\n\nfor (i in models_name){\n  resp <- generate(i, input_prompt)\n  #print(paste0(\"Model: \", i))\n  print(resp_process(resp, \"text\"))\n  #resp_process(resp, \"df\")\n  all_model=rbind(all_model, resp_process(resp, \"df\"))\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_model\n```\n:::\n\n\n### Run in R with ellmer package\n\n\n### Run in terminal\n\n\n::: {.cell}\n\n```{.python .cell-code}\n!ollama pull llama3.1\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n!ollama run llama3.1 \"tell me a 5-word story\"\n```\n:::\n\n#### Run in Python\n\ninstall package\n\n::: {.cell}\n\n```{.python .cell-code}\n!pip install ollama\n```\n:::\n\nlocal pacakge\n\n::: {.cell}\n\n```{.python .cell-code}\nimport json\nimport pandas as pd\nfrom pandas import json_normalize\n\n\nfrom ollama import chat\nfrom ollama import ChatResponse\nimport ollama\n```\n:::\n\ndownload model\n\n::: {.cell}\n\n```{.python .cell-code}\n#ollama.pull('llama3.2:1b')\n```\n:::\n\n\nlist all download model\n\n::: {.cell}\n\n```{.python .cell-code}\nollama_model=ollama.list()\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Extracting data from the ListResponse\ndata = []\nfor model in ollama_model.models:\n    model_data = {\n        'model': model.model,\n        'modified_at': model.modified_at,\n        'digest': model.digest,\n        'size': (model.size/1000000000),\n        'parent_model': model.details.parent_model,\n        'format': model.details.format,\n        'family': model.details.family,\n        'families': model.details.families,\n        'parameter_size': model.details.parameter_size,\n        'quantization_level': model.details.quantization_level\n    }\n    data.append(model_data)\n\n# Convert the list of dictionaries into a pandas DataFrame\nollama_model_df = pd.DataFrame(data)\n\n# Show the DataFrame\nprint(ollama_model_df)\n```\n:::\n\n\n\n\nslow model detail \n\n::: {.cell}\n\n```{.python .cell-code}\nollama.show('deepseek-r1:7b-qwen-distill-q4_K_M')\n```\n:::\n\n\ndelete model\n\n::: {.cell}\n\n```{.python .cell-code}\n#ollama.delete('llama3.2:1b')\n```\n:::\n\nrun model\n\n::: {.cell}\n\n```{.python .cell-code}\nresponse: ChatResponse=ollama.chat(model='deepseek-r1:7b-qwen-distill-q4_K_M', messages=[\n  {'role': 'system', \n  'content': '你是一个诗人，你只能输出中文'},\n  \n  {'role': 'assistant', \n  'content': ''},\n  \n  {'role': 'user', \n  'content': 'give me a 3 lines story'}\n  ])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(response.message.content)\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresponse: ChatResponse =ollama.chat(model='gemma3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(response.message.content)\n```\n:::\n\n\ncreate model \n\n\n::: {.cell}\n\n```{.python .cell-code}\nollama.create(model='example_model', from_='llama3.2', system=\"You are Mario from Super Mario Bros.\")\n```\n:::\n\n\npush model to ollama\n\n\n::: {.cell}\n\n```{.python .cell-code}\nollama.push('user/example_model')\n```\n:::\n:::\n\n# 2.hugging face\n\n![](images/clipboard-1081827289.png)\n\n::: panel-tabset\n\n\n## Run in Python with hugging face transformer\n\nDeepSeek-R1-Distill-Qwen-1.5B as example:\n\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n\n### using pipeline \n::: {.cell}\n\n```{.python .cell-code}\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\n\npipe(messages)\n```\n:::\n\n### Load model directly\n\n::: {.cell}\n\n```{.python .cell-code}\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\ndir_peline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ntext = \"my text for named entity recognition here.\"\n\ndir_peline(text)\n```\n:::\n\n:::\n\n\n\n\n# Terminal\n\n::: panel-tabset\n## run in Terminal\n\nhttps://github.com/allenai/olmocr\n\nhttps://github.com/gradio-app/gradio\n\nhttps://www.youtube.com/watch?v=XF3Q_ZjwfaI\n\n\n\n## run in R code\n\nruning mlx_whisper as example\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommand=paste0(\"mlx_whisper '\",file_name,\"' --model mlx-community/whisper-turbo --language 'Chinese' --initial-prompt '以下是普通話的句子,請以繁體輸出'\")\n\ncommand\n```\n:::\n\n## run in Python code\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\n\ncommand\nos.system(command)\n```\n:::\n:::\n\n# vLLM\n\n# mall pacakge\n\nhttps://mlverse.github.io/mall/\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}