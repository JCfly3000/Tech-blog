{
  "hash": "4e68de20f8d137e907f2d2cedf58d8b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"调用网络端AI模型\"\nsubtitle: \"Run AI model online\"\n\nauthor: \"Tony D\"\ndate: \"2025-03-18\"\n\ncategories: \n  - AI\n  - R\n  - Python\n  \nimage: \"images/S__6840322.jpg\"\n\nexecute:\n  warning: false\n  error: false\n  eval: false\n---\n\nrun LLM model online with ellmer or chatter\n\n# ellmer for R\n\n::: {.cell}\n\n```{.r .cell-code}\n#pak::pkg_install('ellmer')\nlibrary(ellmer)\nlibrary(keyring)\n```\n:::\n## google gemini\n\n\n### gemini-2.0-flash\n::: {.cell}\n\n```{.r .cell-code}\nchat_gemini_model=chat_gemini(\n  system_prompt = NULL,\n  turns = NULL,\n  base_url = \"https://generativelanguage.googleapis.com/v1beta/\",\n  api_key = key_get(\"google_ai_api_key\"),\n  model = \"gemini-2.0-flash\",\n  api_args = list(),\n  echo = NULL\n)\n\nchat_gemini_model\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat_gemini_model$chat(\"Tell me three jokes about statisticians\")\n```\n:::\n\n\n\n## ollama on local\n\n### set up ollama local\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ollamar)\nollamar::pull(\"llama3.1\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nollamar::list_models()\n```\n:::\n\n### difine model\n\n::: {.cell}\n\n```{.r .cell-code}\nchat=chat_ollama(\n  system_prompt = NULL,\n  turns = NULL,\n  base_url = \"http://localhost:11434\",\n  model=\"llama3.1\",\n  seed = NULL,\n  api_args = list(),\n  echo = NULL\n)\n\nchat$get_model()\n```\n:::\n\n\n### run LLM\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$chat(\"Tell me three jokes about statisticians\")\n```\n:::\n\n### run on console\n\n::: {.cell}\n\n```{.r .cell-code}\nlive_console(chat)\n```\n:::\n\n![](images/clipboard-52512780.png){width=\"700\"}\n\n### check token usage\n\n::: {.cell}\n\n```{.r .cell-code}\ntoken_usage()\n```\n:::\n\n\n\n\n# chattr LLM pacakge for R\n\n## Step 1 Install package\n\n::: {.cell}\n\n```{.r .cell-code}\n#remotes::install_github(\"mlverse/chattr\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(chattr)\n```\n:::\n\n# Step 2 set key\n\n-   Login at https://platform.openai.com/\n\n-   Goto Settings (gear icon on top right)\n\n-   Find API Keys from menu on left\n\n-   Follow the process to Create new secret key\n\n-   Copy your secret key (it will only show once so make sure you copy it)\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(OpenAI_API_KEY=\"sk-xxxxxxxx\")\n```\n:::\n\n## Step 3 run ChatGPT as background job\n\n### select model\n\n::: {.cell}\n\n```{.r .cell-code}\n#copilot do not need OpenAI_API_KEY\nchattr_use(\"copilot\")\n```\n:::\n\n### add prompt\n\n::: {.cell}\n\n```{.r .cell-code}\nchattr_defaults(prompt = \"{readLines(system.file('prompt/base.txt', package = 'chattr'))}\")\n```\n:::\n\n### run ChatGPT as background jobs\n\nDo not use Copilot (GitHub) model for chattr(). Github will block this behavior.\n\n::: {.cell}\n\n```{.r .cell-code}\n# run \nchattr_app(as_job = TRUE)\n```\n:::\n\n\nDone!\n\n![](https://blogs.rstudio.com/ai/posts/2024-04-04-chat-with-llms-using-chattr/images/buttons.png)\n\n## Or setup auto open Chat GPT when Rstudio start\n\n### Step 1 find Rprofile file\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"usethis\")  # Install if not already installed\nusethis::edit_r_profile()\n```\n:::\n\n### Step 2 edit Rprofile file as below\n\n::: {.cell filename='.RProfile'}\n\n```{.r .cell-code}\n#|eval: false\n# Load chattr app after RStudio is fully loaded\nsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession) {\n    Sys.sleep(2)  # Wait 2 seconds before starting chattr to ensure RStudio is ready\n    tryCatch({\n      library(chattr)\n      chattr_use(\"copilot\")\n      #Sys.setenv(\"OPENAI_API_KEY\" = \"your-api-key-here\")\n      chattr_defaults(prompt = \"{readLines(system.file('prompt/base.txt', package = 'chattr'))}\")\n\n      chattr_app(as_job = TRUE)\n    }, error = function(e)\n      message(\"Error starting chattr: \", e$message))\n  }\n}, action = \"append\")\n```\n:::\n\n\n\n# google python API\n::: {.cell}\n\n```{.bash .cell-code}\npip install -q -U google-genai\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom google import genai\n\nclient = genai.Client(api_key=google_ai_key_py)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n)\nprint(response.text)\n```\n:::\n\n\n\n# chatlas for python\n\nhttps://github.com/posit-dev/chatlas\n\n\n::: {.cell}\n\n```{.bash .cell-code}\npip install -U chatlas\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom chatlas import ChatGoogle\nfrom chatlas import ChatOllama\nfrom chatlas import token_usage\nimport keyring\n```\n:::\n\n\n## gemini model\n::: {.cell}\n\n```{.python .cell-code}\nfrom chatlas import ChatGoogle\n\nchat_google_model = ChatGoogle(\n  model = \"gemini-2.0-flash\",\n  api_key=keyring.get_password(\"system\", \"google_ai_api_key\"),\n  system_prompt = \"You are a whisky expert\",\n)\n\nchat_google_model\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchat_google_model.chat(\"translate following whisky tasting note to English:微酸，脏麦芽。菲特肯还是要找1988\")\n```\n:::\n\n\n\n## local Ollama model\n::: {.cell}\n\n```{.python .cell-code}\nfrom chatlas import ChatOllama\n\nchat_llama_model = ChatOllama(\n  model=\"llama3.2\",\n  #api_key=keyring.get_password(\"system\", \"google_ai_api_key\"),\n  system_prompt = \"You are a whisky expert\",\n)\n\nchat_llama_model\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchat_llama_model.chat(\"translate following whisky tasting note to English:微酸，脏麦芽。菲特肯还是要找1988\")\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntoken_usage()\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}